---
title: "Real vs Fake News"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

DONE

# Literature Review

DONE

# Hypothesis or Reasearch Question

DONE

# Data and Variables

```{r warning=FALSE}
# libraries 
library(quanteda)
library(dplyr)
library(tidytext)
library(tm)
library(topicmodels)
library(data.table)
library(ggplot2)
library(gridExtra)
library(rvest)
library(stringr)
library(tidyverse)
```

## Data 

Data Source https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k 

```{r}
# import the data
fake_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/Fake.csv?raw=true")
real_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/True.csv?raw=true")
#fake_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_FAKE.csv")
#real_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_TRUE.csv")
```

```{r}
# preview raw data
head(fake_df)
head(real_df)
```

```{r}
fake_df <- fake_df %>%
  select(title, text, subject)
print("Summary of 'Fake' Dataset")
summary(fake_df)

real_df <- real_df %>%
  select(title, text, subject)
print("Summary of 'Real' Dataset")
summary(real_df)
#str(real_df)
```


Other summaries
```{r}
str(fake_df)
str(real_df)


#fake_df %>%
#  count(text, sort = TRUE)

#real_df %>%
#  count(text, sort = TRUE)
```

Missing Values
```{r}
print("'Fake' Dataset Missing Values")
sum(is.na(fake_df))
print("'Real' Dataset Missing Values")
sum(is.na(real_df))
```



```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- fake_words %>% anti_join(stop_words, by = "word")

# count each word
fake_count <- fake_words %>% 
  count(word, sort = TRUE)
head(fake_count)
```

```{r}
# clean the data  using tidytext
real_words <- real_df %>%
  unnest_tokens(word, text)

# remove stop words
real_words <- real_words %>% anti_join(stop_words, by = "word")

# count each word
real_count <- real_words %>% 
  count(word, sort = TRUE)

#remove weird character
real_count <- real_count %>%
  filter(word != 'Ã¢')

head(real_count)
```

chart of words
```{r fig.height=8, fig.width=15}
p1 <- head(fake_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Fake News Top Words",
         y = "count") +
  theme(legend.position = "none")


p2 <- head(real_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Real News Top Words",
         y = "count") +
  theme(legend.position = "none")

grid.arrange(p1, p2, ncol = 2)
```



Normalizing the data by case-folding, stopword removal, stamming, lemmatization, and contraction simplification
```{r}
# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
fake_df.corpus <- Corpus(VectorSource(fake_df))

space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
fake.corpus <- tm_map(fake_df.corpus, space, "/")
fake.corpus <- tm_map(fake.corpus, space, "@")
fake.corpus <- tm_map(fake.corpus, space, "\\|")
#remove stopwords
#fake.corpus <- tm_map(fake_df.corpus, removeWords, stopwords("english"))
# convert to lower
fake.corpus <- tm_map(fake_df.corpus, content_transformer(tolower))
#remove numbers
fake.corpus <- tm_map(fake_df.corpus, removeNumbers)
#remove punctuations
fake.corpus <- tm_map(fake_df.corpus, removePunctuation)
#strip white spaces
fake.corpus <- tm_map(fake_df.corpus, stripWhitespace)
# stemming
fake.corpus <- tm_map(fake.corpus, content_transformer(stemDocument))

head(fake.corpus)
```

Create Document Term Matrix (DTM)
```{r}
# first DTM
fake.dtm <- DocumentTermMatrix(fake.corpus)
fake.dtm <- as.matrix(fake.dtm)
# remove low frequency words (sparse terms)
#fake.dtm2 <- removeSparseTerms(fake.dtm, sparse = 0.99)
#inspect(fake.dtm2)
#??inspect
#convert to dataframe
fake.dtm.sort <- sort(rowSums(fake.dtm), decreasing = TRUE)
fake.dtm.sort <- as.data.frame(word = names(fake.dtm.sort), freq = fake.dtm.sort)
fake.dtm
#write.csv(fake.dtm.df,"DTM.csv")
```

```{r}
# sort by decreasing order
x <- mapply(sum, fake.dtm.df)
colSums(fake.dtm.df)
fake.dtm.sort <- data.frame(fake.dtm.sort)
fake.dtm.sort <- order(fake.dtm.sort, decreasing = TRUE)

fake.dtm.sort
#transpose df
summary(fake.dtm.sort)
fake.dtm.d <- data.frame(word = names(fake.dtm.sort), freq=fake.dtm.sort)
head(fake.dtm.d)
```


Topic modeling. Using latent Dirichlet allocation (LDA) to learn the essential words in our DTM
```{r}
dtm_s <- sort(rowSums())

# LDA 
fake.lda <- LDA(fake.dtm2, k = 2, control = list(seed = 1234))


?LDA
```

## New York Post Data
The New York Post has a section dedicated to the latest 'Fake News'


Function to loop through 10 NY Post web pages and scrape the news articles off of each page. Each page has 20 articles. The first fuction scrapes the article title, publication date, and the URL to the individual article. It then puts this scraped data into a dataframe. 
```{r}
article_list <- data.frame()
temp_list <- data.frame()
max <- 10
base <- "https://nypost.com/tag/fake-news/page/"

for(i in 1:max){
    temp <- paste(base, as.character(i), sep = "")
    url <- read_html(temp)
    
    #get all article titles
    a <- url %>%
      html_nodes(".story__headline.headline.headline--archive") %>%
      html_text(trim = TRUE)
    
    #get article dates
    b <- url %>%
      html_nodes("span.meta.meta--byline") %>%
      html_text(trim = TRUE)
    
    #get article URLs
    c <- url %>%
      html_nodes(".headline--archive a") %>%
      html_attr("href") %>%
      as.character()
    
    #dataframe of scraped articles
    temp_list <- cbind(a,b,c)
    article_list <- rbind(article_list,temp_list)
    
}
```

The second function loops through each link on the first dataframe, scrapes the article text, then adds it to the dataframe. The final data frame has one for per article and the columns are the title, date, URL, and article text. 
```{r}
# get article text for each row

all_text <- data.frame(matrix(ncol = 1,nrow = 0))
colnames(all_text) <- c('text')
#temp_list <- data.frame()

for(i in 1:nrow(article_list)){
  temp <- read_html(article_list[i,3])
  
  #get text
  a <- temp %>%
    html_node("div.single__content.entry-content.m-bottom") %>%
    html_text(trim = TRUE)
  
  #get tag list
  #article 1 tag list
  #b <- temp %>%
   # html_nodes("li.tag-list__tag") %>%
    #html_text(trim = TRUE)
  
 # temp_list <- cbind(temp_list,a,b)
  all_text <- rbind(all_text,a)
}
```

```{r}
#final dataframe

master_list <- cbind(article_list, all_text)
colnames(master_list) <- c('title', 'date','url','text')
head(master_list)
```

Analysis of Titles 
```{r}
master_list_titles <- master_list %>%
  select(title)
str(master_list_titles)
head(master_list_titles)
```

Clean Title text
```{r}
# clean the data  using tidytext
title_words <- master_list_titles %>%
  unnest_tokens(word, title)

# remove stop words
title_words <- title_words %>% 
  anti_join(stop_words, by = "word")


# count each word
title_count <- title_words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
title_count <- title_count %>%
  filter(word != c('fake','news'))
head(title_count)
```

Analysis of Date column
```{r}
master_list_date <- master_list %>%
  select(date)
str(master_list_date)
head(master_list_date)
```

Split Date column to remove time and keep date only
```{r}
master_list_date <- master_list_date %>%
  separate(date, c("month","day","year")," ")

head(master_list_date)

year_count <- master_list_date %>% 
  count(year, sort = TRUE)

head(year_count)
```

combine month and year. then take a count
```{r}
master_list_date$month_year <- paste(master_list_date$month, master_list_date$year, sep = " ")
head(master_list_date)

month_count <- master_list_date %>% 
  count(month_year, sort = TRUE)

head(month_count)
```



Analysis of Article text
```{r}
master_list_text <- master_list %>%
  select(text)
str(master_list_text)
head(master_list_text)
```

```{r}
# clean the data  using tidytext
words <- master_list_text %>%
  unnest_tokens(word, text)

# remove stop words
words <- words %>% 
  anti_join(stop_words, by = "word")


# count each word
words_count <- words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
words_count <- words_count %>%
  filter(word != c('fake','news'))

head(words_count)
```


# Statistical Methods



# Reference



# Appendix


# Results


# Conclusion



# Abstract






