---
title: "Real vs Fake News"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

DONE

# Literature Review

DONE

# Hypothesis or Reasearch Question

DONE

# Data and Variables

```{r warning=FALSE, message=FALSE}
# libraries 
library(quanteda)
library(dplyr)
library(tidytext)
library(tm) # for text mining
library(topicmodels)
library(data.table)
library(ggplot2)
library(gridExtra)
library(rvest)
library(stringr)
library(tidyverse)
library(tidyr)
library(SnowballC) # for text stemming
library(wordcloud) # word-cloud generator
library(RColorBrewer) # color palettes
library(syuzhet) # for sentiment analysis
```

## Data 

Data Source https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k 

```{r}
# import the data
fake_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/Fake.csv?raw=true")
real_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/True.csv?raw=true")
#fake_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_FAKE.csv")
#real_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_TRUE.csv")
```

```{r}
# preview raw data
head(fake_df)
head(real_df)
```

```{r}
fake_df <- fake_df %>%
  select(title, text, subject)
print("Summary of 'Fake' Dataset")
summary(fake_df)

real_df <- real_df %>%
  select(title, text, subject)
print("Summary of 'Real' Dataset")
summary(real_df)
#str(real_df)
```


Other summaries
```{r}
str(fake_df)
str(real_df)


#fake_df %>%
#  count(text, sort = TRUE)

#real_df %>%
#  count(text, sort = TRUE)
```

Missing Values
```{r}
print("'Fake' Dataset Missing Values")
sum(is.na(fake_df))
print("'Real' Dataset Missing Values")
sum(is.na(real_df))
```



```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- fake_words %>% anti_join(stop_words, by = "word")

# count each word
fake_count <- fake_words %>% 
  count(word, sort = TRUE)
head(fake_count)
```

```{r}
# clean the data  using tidytext
real_words <- real_df %>%
  unnest_tokens(word, text)

# remove stop words
real_words <- real_words %>% anti_join(stop_words, by = "word")

# count each word
real_count <- real_words %>% 
  count(word, sort = TRUE)

#remove weird character
real_count <- real_count %>%
  filter(word != 'Ã¢')

head(real_count)
```

chart of words
```{r fig.height=8, fig.width=15}
p1 <- head(fake_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Fake News Top Words",
         y = "count") +
  theme(legend.position = "none")


p2 <- head(real_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Real News Top Words",
         y = "count") +
  theme(legend.position = "none")

grid.arrange(p1, p2, ncol = 2)
```



Normalizing the data by case-folding, stopword removal, stamming, lemmatization, and contraction simplification
```{r}
# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
fake_df.corpus <- Corpus(VectorSource(fake_df))

space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
fake.corpus <- tm_map(fake_df.corpus, space, "/")
fake.corpus <- tm_map(fake.corpus, space, "@")
fake.corpus <- tm_map(fake.corpus, space, "\\|")
#remove stopwords
#fake.corpus <- tm_map(fake_df.corpus, removeWords, stopwords("english"))
# convert to lower
fake.corpus <- tm_map(fake_df.corpus, content_transformer(tolower))
#remove numbers
fake.corpus <- tm_map(fake_df.corpus, removeNumbers)
#remove punctuations
fake.corpus <- tm_map(fake_df.corpus, removePunctuation)
#strip white spaces
fake.corpus <- tm_map(fake_df.corpus, stripWhitespace)
# stemming
fake.corpus <- tm_map(fake.corpus, content_transformer(stemDocument))

head(fake.corpus)
```

Create Document Term Matrix (DTM)
```{r}
# first DTM
fake.dtm <- DocumentTermMatrix(fake.corpus)
fake.dtm <- as.matrix(fake.dtm)
# remove low frequency words (sparse terms)
#fake.dtm2 <- removeSparseTerms(fake.dtm, sparse = 0.99)
#inspect(fake.dtm2)
#??inspect
#convert to dataframe
fake.dtm.sort <- sort(rowSums(fake.dtm), decreasing = TRUE)
fake.dtm.sort <- as.data.frame(word = names(fake.dtm.sort), freq = fake.dtm.sort)
fake.dtm
#write.csv(fake.dtm.df,"DTM.csv")
```

```{r}
# sort by decreasing order
x <- mapply(sum, fake.dtm.df)
colSums(fake.dtm.df)
fake.dtm.sort <- data.frame(fake.dtm.sort)
fake.dtm.sort <- order(fake.dtm.sort, decreasing = TRUE)

fake.dtm.sort
#transpose df
summary(fake.dtm.sort)
fake.dtm.d <- data.frame(word = names(fake.dtm.sort), freq=fake.dtm.sort)
head(fake.dtm.d)
```


Topic modeling. Using latent Dirichlet allocation (LDA) to learn the essential words in our DTM
```{r}
dtm_s <- sort(rowSums())

# LDA 
fake.lda <- LDA(fake.dtm2, k = 2, control = list(seed = 1234))


?LDA
```

## New York Post Data
The New York Post has a section dedicated to the latest 'Fake News'


Function to loop through 10 NY Post web pages and scrape the news articles off of each page. Each page has 20 articles. The first fuction scrapes the article title, publication date, and the URL to the individual article. It then puts this scraped data into a dataframe. 
```{r}
article_list <- data.frame()
temp_list <- data.frame()
max <- 10
base <- "https://nypost.com/tag/fake-news/page/"

for(i in 1:max){
    temp <- paste(base, as.character(i), sep = "")
    url <- read_html(temp)
    
    #get all article titles
    a <- url %>%
      html_nodes(".story__headline.headline.headline--archive") %>%
      html_text(trim = TRUE)
    
    #get article dates
    b <- url %>%
      html_nodes("span.meta.meta--byline") %>%
      html_text(trim = TRUE)
    
    #get article URLs
    c <- url %>%
      html_nodes(".headline--archive a") %>%
      html_attr("href") %>%
      as.character()
    
    #dataframe of scraped articles
    temp_list <- cbind(a,b,c)
    article_list <- rbind(article_list,temp_list)
    
}
```

The second function loops through each link on the first dataframe, scrapes the article text, then adds it to the dataframe. The final data frame has one for per article and the columns are the title, date, URL, and article text. 
```{r}
# get article text for each row

all_text <- data.frame(matrix(ncol = 1,nrow = 0))
colnames(all_text) <- c('text')
#temp_list <- data.frame()

for(i in 1:nrow(article_list)){
  temp <- read_html(article_list[i,3])
  
  #get text
  a <- temp %>%
    html_node("div.single__content.entry-content.m-bottom") %>%
    html_text(trim = TRUE)
  
  #get tag list
  #article 1 tag list
  #b <- temp %>%
   # html_nodes("li.tag-list__tag") %>%
    #html_text(trim = TRUE)
  
 # temp_list <- cbind(temp_list,a,b)
  all_text <- rbind(all_text,a)
}
```


```{r}
#final dataframe

master_list <- cbind(article_list, all_text)
colnames(master_list) <- c('title', 'date','url','text')
head(master_list)
```

Analysis of Titles 
```{r}
master_list_titles <- master_list %>%
  select(title)
str(master_list_titles)
head(master_list_titles)
```

Clean Title text
```{r}
# clean the data  using tidytext
title_words <- master_list_titles %>%
  unnest_tokens(word, title)

# remove stop words
title_words <- title_words %>% 
  anti_join(stop_words, by = "word")


# count each word
title_count <- title_words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
title_count <- title_count %>%
  filter(word != c('fake','news'))
head(title_count)
```

Analysis of Date column
```{r}
master_list_date <- master_list %>%
  select(date)
str(master_list_date)
head(master_list_date)
```

Split Date column to remove time and keep date only. Just looking at this New York post data it looks like articles have decreased over the years. 
```{r}
master_list_date <- master_list_date %>%
  separate(date, c("month","day","year")," ")

head(master_list_date)

year_count <- master_list_date %>% 
  count(year, sort = TRUE)

head(year_count)
```

combine month and year. then take a count
```{r}
master_list_date$month_year <- paste(master_list_date$month, master_list_date$year, sep = " ")
head(master_list_date)

month_count <- master_list_date %>% 
  count(month_year, sort = TRUE)

head(month_count)
```



Analysis of Article text
```{r}
master_list_text <- master_list %>%
  select(text)
str(master_list_text)
head(master_list_text)
```

```{r}
# clean the data  using tidytext
words <- master_list_text %>%
  unnest_tokens(word, text)


# remove stop words
words <- words %>% 
  anti_join(stop_words, by = "word")


# count each word
words_count <- words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
words_count <- words_count %>%
  filter(word != c('fake','news'))

head(words_count)
tail(words_count)
```

Sentiment analysis of NYP words
```{r}
words_sentiment <- words_count %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

??linenumber
```

Prep data for sentiment analysis
```{r}
# load data as a corpus
textDoc <- Corpus(VectorSource(master_list_text))

# replace "/", "@", and "|" with space
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
textDoc <- tm_map(textDoc, toSpace, "/")
textDoc <- tm_map(textDoc, toSpace, "@")
textDoc <- tm_map(textDoc, toSpace, "\\|")

# convert the text to lower case
textDoc <- tm_map(textDoc, content_transformer(tolower))
# remove numbers
textDoc <- tm_map(textDoc, removeNumbers)
# remove english common stopwords
textDoc <- tm_map(textDoc, removeWords, stopwords("english"))
# remove punctuation
textDoc <- tm_map(textDoc, removePunctuation)
# eliminate extra white spaces
textDoc <- tm_map(textDoc, stripWhitespace)
# text stemming - reduce words to their root form
textDoc <- tm_map(textDoc, stemDocument)
# remove custom words after seeing the frequency
removeSpecialChars <- function(x) gsub("-","",x)
textDoc <- tm_map(textDoc, removeSpecialChars)
# remove custom words 
textDoc <- tm_map(textDoc, removeWords, c("said","news","fake","post","''"))
```

Build Document Term Matrix (DTM)
```{r}
textDoc_dtm <- TermDocumentMatrix(textDoc)
dtm_m <- as.matrix(textDoc_dtm)
# sort by decreasing value of frequency
dtm_v <- sort(rowSums(dtm_m), decreasing = TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
# remove first two rows
dtm_d <- dtm_d[-c(1:2),]
# remove last 5 rows
dtm_d <- slice(dtm_d, 1:(n() - 6))
# display the top 5 most frequent works
head(dtm_d)
tail(dtm_d)
```

plot most frequent words
```{r}
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col = "lightgreen", main = "Top 5 most frequent words", 
        ylab = "Word Frequencies")
```

Generate word cloud
```{r}
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
```

Find word association
```{r}
findAssocs(textDoc_dtm, terms = c("report","say","trump"), corlimit = 0.25)
```

Find associations for words that occur at least 50 times
```{r}
findAssocs(textDoc_dtm, terms = findFreqTerms(textDoc_dtm, lowfreq = 50), corlimit = 0.25)
```

Sentiment Scores
```{r}
# clean raw file before getting sentiment scores
clean_master_list <- master_list_text
clean_master_list$text <- gsub("\\n"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("\\t"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("More On:"," ",as.character(clean_master_list$text))
head(clean_master_list)
```

```{r}
syuzhet_vector <- get_sentiment(clean_master_list$text, method = "syuzhet")
bing_vector <- get_sentiment(clean_master_list$text, method = "bing")
afinn_vector <- get_sentiment(clean_master_list$text, method = "afinn")
summary(syuzhet_vector)
summary(bing_vector)
summary(afinn_vector)
```

Emotional classification
```{r}
d <- get_nrc_sentiment(clean_master_list$text)
head(d)
```

Graph the emotional classification results
```{r}
#transpose
td <- data.frame(t(d))
# compute sums of each column
td_new <- data.frame(rowSums(td[2:200]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2 <- td_new[1:10,]
quickplot(sentiment, data = td_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("Survey Sentiments")
```

% of emotions
```{r}
barplot(
  sort(colSums(prop.table(d[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in New York Post", xlab = "Percentage",
)

```

N-gram analysis
```{r}
?bigram
```




# Statistical Methods

For this topic modeling, we will use and Latent Dirichlet Allocation (LDA) model.
```{r}
inspect(dtm_d)
```







# Results

```{r}

```




# Conclusion

```{r}

```




# Abstract

# Reference



# Appendix






