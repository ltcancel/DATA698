---
title: "Real vs Fake News"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction


# Literature Review



# Hypothesis or Reasearch Question


# Data and Variables

```{r}
# libraries
library(quanteda)
library(dplyr)
library(tidytext)
library(tm)
library(topicmodels)
```


```{r}
# import the data
fake_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/Fake.csv?raw=true")
real_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/True.csv?raw=true")
```

```{r}
# preview raw data
head(fake_df)
head(real_df)
```

```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- anti_join(fake_words, get_stopwords())

# count each word
fake_words <- fake_words %>% 
  count(word, sort = TRUE)

head(fake_words)
tail(fake_words)
```

Normalizing the data by case-folding, stopword removal, stamming, lemmatization, and contraction simplification
```{r}
# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
fake_df.corpus <- Corpus(VectorSource(fake_df$text))
# case-folding
fake.corpus <- tm_map(fake_df.corpus, tolower)
# stemming
fake.corpus <- tm_map(fake.corpus, stemDocument)

head(fake.corpus)
```

Create Document Term Matrix (DTM)
```{r}
# first DTM
fake.dtm <- DocumentTermMatrix(fake.corpus, control = list(minWordLength = 3))
fake.dtm
# remove low frequency workds (sparse terms)
fake.dtm2 <- removeSparseTerms(fake.dtm, sparse = 0.98)
fake.dtm2
```

Topic modeling. Using latent Dirichlet allocation (LDA) to learn the essential words in our DTM
```{r}
# LDA 
fake.lda <- LDA(fake.dtm2, k = 2, control = list(seed = 1234))


?LDA
```


# Statistical Methods



# Reference



# Appendix


# Results


# Conclusion



# Abstract






