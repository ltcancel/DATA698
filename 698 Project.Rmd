---
title: "Real vs Fake News"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

DONE

# Literature Review

DONE

# Hypothesis or Reasearch Question

DONE

# Data and Variables

```{r warning=FALSE, message=FALSE}
# libraries 
library(quanteda)
library(dplyr)
library(tidytext)
library(tm) # for text mining
library(topicmodels)
library(data.table)
library(ggplot2)
library(gridExtra)
library(rvest)
library(stringr)
library(tidyverse)
library(tidyr)
library(SnowballC) # for text stemming
library(wordcloud) # word-cloud generator
library(RColorBrewer) # color palettes
library(syuzhet) # for sentiment analysis
library(generics)
library(reshape2)
library(textdata)
```

## Data 

Data Source https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k 

```{r}
# import the data
fake_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/Fake.csv?raw=true")
real_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/True.csv?raw=true")
#fake_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_FAKE.csv")
#real_df <- read.csv("https://github.com/ltcancel/DATA698/raw/main/Data/DataSet_Misinfo_TRUE.csv")
```

```{r}
# preview raw data
head(fake_df)
head(real_df)
```

```{r}
fake_df <- fake_df %>%
  select(title, text, subject)
print("Summary of 'Fake' Dataset")
summary(fake_df)

real_df <- real_df %>%
  select(title, text, subject)
print("Summary of 'Real' Dataset")
summary(real_df)

summary(nyp)
```


Other summaries
```{r}
str(fake_df)
str(real_df)
```

Missing Values
```{r}
print("'Fake' Dataset Missing Values")
sum(is.na(fake_df))
print("'Real' Dataset Missing Values")
sum(is.na(real_df))
```

Subset each DF to first 10,000 rows to cut down on the amount of time it takes R to process the information. print new summary
```{r}
fake_df <- fake_df[1:5000,] 
real_df <- real_df[1:5000,]

fake_df <- fake_df %>%
  select(title, text, subject)
print("Summary of 'Fake' Dataset")
summary(fake_df)

real_df <- real_df %>%
  select(title, text, subject)
print("Summary of 'Real' Dataset")
summary(real_df)


print("Summary of NYP Dataset")
summary(master_list)
```


```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- fake_words %>% anti_join(stop_words, by = "word")

# count each word
fake_count <- fake_words %>% 
  count(word, sort = TRUE)
head(fake_count)
```

```{r}
# clean the data  using tidytext
real_words <- real_df %>%
  unnest_tokens(word, text)

# remove stop words
real_words <- real_words %>% anti_join(stop_words, by = "word")

# count each word
real_count <- real_words %>% 
  count(word, sort = TRUE)

#remove weird character
real_count <- real_count %>%
  filter(word != 'Ã¢')

head(real_count)
```




chart of words
```{r fig.height=8, fig.width=15}
p1 <- head(fake_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Fake News Top Words",
         y = "count") +
  theme(legend.position = "none")


p2 <- head(real_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Real News Top Words",
         y = "count") +
  theme(legend.position = "none")

grid.arrange(p1, p2, ncol = 2)
```

## Document Term Matrix

###Fake News

```{r}
# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
fake_df.corpus <- Corpus(VectorSource(fake_df$text))

space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
fake.corpus <- tm_map(fake_df.corpus, space, "/")
fake.corpus <- tm_map(fake.corpus, space, "@")
fake.corpus <- tm_map(fake.corpus, space, "\\|")
#remove stopwords
fake.corpus <- tm_map(fake.corpus, removeWords, stopwords("english"))
# convert to lower
fake.corpus <- tm_map(fake.corpus, content_transformer(tolower))
#remove numbers
fake.corpus <- tm_map(fake.corpus, removeNumbers)
#remove punctuations
fake.corpus <- tm_map(fake.corpus, removePunctuation)
#strip white spaces
fake.corpus <- tm_map(fake.corpus, stripWhitespace)
# stemming
fake.corpus <- tm_map(fake.corpus, content_transformer(stemDocument))

# remove custom words after seeing the frequency
removeSpecialChars <- function(x) gsub("@","",x)
fake.corpus <- tm_map(fake.corpus, removeSpecialChars)
# remove custom words 
fake.corpus <- tm_map(fake.corpus, removeWords, c("the","and","for","that","with","was","this",")","(","has","you","are"))
```
Fake Dataset Document Term Matrix
```{r}
fakeDoc_dtm <- TermDocumentMatrix(fake.corpus)
dtm_mf <- as.matrix(fakeDoc_dtm)
# sort by decreasing value of frequency
dtm_vf <- sort(rowSums(dtm_mf), decreasing = TRUE)
dtm_df <- data.frame(word = names(dtm_vf), freq=dtm_vf)
# remove additional words
#dtm_df <- dtm_df[-c(1:2),]
# remove last 5 rows
#dtm_df <- slice(dtm_df, 1:(n() - 6))
# display the top 5 most frequent works
head(dtm_df)
tail(dtm_df)
```

## Fake sentiment analysis

Sentiment Scores NYP
```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- fake_words %>% anti_join(stop_words, by = "word")

# count each word
fake_count <- fake_words %>% 
  count(word, sort = TRUE)
head(fake_count)

# clean raw file before getting sentiment scores
clean_master_list <- fake_df
clean_master_list$text <- gsub("\\n"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("\\t"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("More On:"," ",as.character(clean_master_list$text))
head(clean_master_list)
head(fake_df)
```

Sentiment scores for Fake data
```{r}
syuzhet_fake <- get_sentiment(fake_df$text, method = "syuzhet")
bing_fake <- get_sentiment(fake_df$text, method = "bing")
afinn_fake <- get_sentiment(fake_df$text, method = "afinn")
summary(syuzhet_fake)
summary(bing_fake)
summary(afinn_fake)
```

## Emotional classification fake
```{r}
f <- get_nrc_sentiment(fake_df$text)
head(f)
```

Graph the emotional classification results
```{r}
#transpose
tdf <- data.frame(t(f))
# compute sums of each column
tdf_new <- data.frame(rowSums(tdf[2:5000]))
names(tdf_new)[1] <- "count"
tdf_new <- cbind("sentiment" = rownames(tdf_new), tdf_new)
rownames(tdf_new) <- NULL
tdf_new2 <- tdf_new[1:10,]
quickplot(sentiment, data = tdf_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("Fake News Sentiments")
q1 <- quickplot(sentiment, data = tdf_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("Fake News Sentiments")
```

## % of emotions fake data
```{r}
barplot(
  sort(colSums(prop.table(f[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in Fake News Dataset", xlab = "Percentage",
)

```

## Document Term Matrix

### Real Datasest

Prep data for sentiment analysis
```{r}
# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
real_df.corpus <- Corpus(VectorSource(real_df$text))

space <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
real.corpus <- tm_map(real_df.corpus, space, "/")
real.corpus <- tm_map(real.corpus, space, "@")
real.corpus <- tm_map(real.corpus, space, "\\|")
#remove stopwords
real.corpus <- tm_map(real.corpus, removeWords, stopwords("english"))
# convert to lower
real.corpus <- tm_map(real.corpus, content_transformer(tolower))
#remove numbers
real.corpus <- tm_map(real.corpus, removeNumbers)
#remove punctuations
real.corpus <- tm_map(real.corpus, removePunctuation)
#strip white spaces
real.corpus <- tm_map(real.corpus, stripWhitespace)
# stemming
real.corpus <- tm_map(real.corpus, content_transformer(stemDocument))

# remove custom words after seeing the frequency
removeSpecialChars <- function(x) gsub("@","",x)
real.corpus <- tm_map(real.corpus, removeSpecialChars)
# remove custom words 
real.corpus <- tm_map(real.corpus, removeWords, c("the","and","for","that","with","was","this",")","(","has","you","are"))
```


Real Dataset Document Term Matrix
```{r}
realDoc_dtm <- TermDocumentMatrix(real.corpus)
dtm_mr <- as.matrix(realDoc_dtm)
# sort by decreasing value of frequency
dtm_vr <- sort(rowSums(dtm_mr), decreasing = TRUE)
dtm_dr <- data.frame(word = names(dtm_vr), freq=dtm_vr)
# remove additional words
#dtm_df <- dtm_df[-c(1:2),]
# remove last 5 rows
#dtm_df <- slice(dtm_df, 1:(n() - 6))
# display the top 5 most frequent works
head(dtm_dr)
tail(dtm_dr)
```

## Real Data sentiment analysis

Sentiment scores for Real data
```{r}
syuzhet_real <- get_sentiment(real_df$text, method = "syuzhet")
bing_real <- get_sentiment(real_df$text, method = "bing")
afinn_real <- get_sentiment(real_df$text, method = "afinn")
summary(syuzhet_real)
summary(bing_real)
summary(afinn_real)
```

## Emotional classification fake
```{r}
r <- get_nrc_sentiment(real_df$text)
head(r)
```

Graph the emotional classification results
```{r}
#transpose
tdr <- data.frame(t(r))
# compute sums of each column
tdr_new <- data.frame(rowSums(tdr[2:5000]))
names(tdr_new)[1] <- "count"
tdr_new <- cbind("sentiment" = rownames(tdr_new), tdr_new)
rownames(tdr_new) <- NULL
tdr_new2 <- tdr_new[1:10,]
quickplot(sentiment, data = tdr_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("Real News Sentiments")
q2 <- quickplot(sentiment, data = tdr_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("Real News Sentiments")
```

## % of emotions real data
```{r}
barplot(
  sort(colSums(prop.table(r[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in Real News Dataset", xlab = "Percentage",
)

```

## Topic Modeling Fake News

```{r}
head(fake_df)

f_tokens <- fake_df$text %>%
  tokens(what = "word",
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>%
  tokens_remove(pattern = c("s", "t", "image","images","getty"))

dfm_f <- dfm_trim(dfm(f_tokens), min_termfreq = 0.005, max_docfreq = 0.99,
                docfreq_type = "prop", verbose = TRUE)

dfm_f
```

Check the top 20 features
```{r}
topfeatures(dfm_f, n=50, scheme = "docfreq")
```

Remove additional words and check top features again. 
```{r}
dfm_f <- dfm_remove(dfm_f, c("re","go","said","can","o","$","=","j"))
topfeatures(dfm_f, n=50, scheme = "docfreq")
```

# LDA Statistical Methods Fake

## LDA model for fake news

```{r}
dtm_f <- convert(dfm_f, to = "topicmodels")
set.seed(1)
mf <- LDA(dtm_f, method = "Gibbs", k = 15, control = list(alpha = 0.1))
#n <- LDA(dtm, method = "Gibbs", k = 15, control = list(seed=1234))
mf
#n
```

Inspecting the LDA results by looking at the top 5 terms for 10 Topics
```{r}
terms(mf, 5)
#terms(n,5)
```

Visualization of the top topics
```{r}
#tidy_lda <- as.data.frame(terms(m, dim(m)[1]))
f_tidy_lda <- tidy(mf)
f_tidy_lda
```

```{r}
top_terms_f <- f_tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_f
```


```{r fig.height=15, fig.width=10}
top_terms_f %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - Fake News Dataset", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```

## Topic Modeling Real News

```{r}
head(real_df)

r_tokens <- real_df$text %>%
  tokens(what = "word",
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) 

dfm_r <- dfm_trim(dfm(r_tokens), min_termfreq = 0.005, max_docfreq = 0.99,
                docfreq_type = "prop", verbose = TRUE)

dfm_r
```

Check the top 20 features
```{r}
topfeatures(dfm_r, n=50, scheme = "docfreq")
```

Remove additional words and check top features again. 
```{r}
dfm_r <- dfm_remove(dfm_r, c("$"))
topfeatures(dfm_r, n=50, scheme = "docfreq")
```

# LDA Statistical Methods Real

## LDA model for Real news

```{r}
dtm_r <- convert(dfm_r, to = "topicmodels")
set.seed(1)
mr <- LDA(dtm_r, method = "Gibbs", k = 15, control = list(alpha = 0.1))
#n <- LDA(dtm, method = "Gibbs", k = 15, control = list(seed=1234))
mr
#n
```

Inspecting the LDA results by looking at the top 5 terms for 10 Topics
```{r}
terms(mr, 5)
#terms(n,5)
```

Visualization of the top topics
```{r}
#tidy_lda <- as.data.frame(terms(m, dim(m)[1]))
r_tidy_lda <- tidy(mr)
r_tidy_lda
```

```{r}
top_terms_r <- r_tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_r
```


```{r fig.height=15, fig.width=10}
top_terms_r %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - Real News Dataset", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```



## New York Post Data
The New York Post has a section dedicated to the latest 'Fake News'


Function to loop through 10 NY Post web pages and scrape the news articles off of each page. Each page has 20 articles. The first fuction scrapes the article title, publication date, and the URL to the individual article. It then puts this scraped data into a dataframe. 
```{r}
article_list <- data.frame()
temp_list <- data.frame()
max <- 10
base <- "https://nypost.com/tag/fake-news/page/"

for(i in 1:max){
    temp <- paste(base, as.character(i), sep = "")
    url <- read_html(temp)
    
    #get all article titles
    a <- url %>%
      html_nodes(".story__headline.headline.headline--archive") %>%
      html_text(trim = TRUE)
    
    #get article dates
    b <- url %>%
      html_nodes("span.meta.meta--byline") %>%
      html_text(trim = TRUE)
    
    #get article URLs
    c <- url %>%
      html_nodes(".headline--archive a") %>%
      html_attr("href") %>%
      as.character()
    
    #dataframe of scraped articles
    temp_list <- cbind(a,b,c)
    article_list <- rbind(article_list,temp_list)
    
}
```

The second function loops through each link on the first dataframe, scrapes the article text, then adds it to the dataframe. The final data frame has one for per article and the columns are the title, date, URL, and article text. 
```{r}
# get article text for each row

all_text <- data.frame(matrix(ncol = 1,nrow = 0))
colnames(all_text) <- c('text')
#temp_list <- data.frame()

for(i in 1:nrow(article_list)){
  temp <- read_html(article_list[i,3])
  
  #get text
  a <- temp %>%
    html_node("div.single__content.entry-content.m-bottom") %>%
    html_text(trim = TRUE)
  
  #get tag list
  #article 1 tag list
  #b <- temp %>%
   # html_nodes("li.tag-list__tag") %>%
    #html_text(trim = TRUE)
  
 # temp_list <- cbind(temp_list,a,b)
  all_text <- rbind(all_text,a)
}
```


```{r}
#final dataframe
master_list <- cbind(article_list, all_text)
#update column names
colnames(master_list) <- c('title', 'date','url','text')
#add row number as a column to be used later in a dtm
master_list <- master_list %>%
  mutate(doc=row_number())
head(master_list)
```

Analysis of Titles 
```{r}
master_list_titles <- master_list %>%
  select(doc, title)
str(master_list_titles)
head(master_list_titles)
```

Clean Title text
```{r}
# clean the data  using tidytext
title_words <- master_list_titles %>%
  unnest_tokens(word, title)

# remove stop words
title_words <- title_words %>% 
  anti_join(stop_words, by = "word")


# count each word
title_count <- title_words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
title_count <- title_count %>%
  filter(word != c('fake','news'))
head(title_count)
```

Analysis of Date column
```{r}
master_list_date <- master_list %>%
  select(doc, date)
str(master_list_date)
head(master_list_date)
```

Split Date column to remove time and keep date only. Just looking at this New York post data it looks like articles have decreased over the years. 
```{r}
master_list_date <- master_list_date %>%
  separate(date, c("month","day","year")," ")

head(master_list_date)

year_count <- master_list_date %>% 
  count(year, sort = TRUE)

head(year_count)
```

combine month and year. then take a count
```{r}
master_list_date$month_year <- paste(master_list_date$month, master_list_date$year, sep = " ")
head(master_list_date)

month_count <- master_list_date %>% 
  count(month_year, sort = TRUE)

head(month_count)
```



Analysis of Article text
```{r}
master_list_text <- master_list %>%
  select(text)
str(master_list_text)
head(master_list_text)
```

## Clean article text

Text cleansing and analysis of frequently used words. 
```{r}
# clean the data  using tidytext
words <- master_list_text %>%
  unnest_tokens(word, text)


# remove stop words
words <- words %>% 
  anti_join(stop_words, by = "word")


# count each word
words_count <- words %>% 
  count(word, sort = TRUE)

# remove the words 'fake' and 'news' the count of both words are inflated since the web page is dedicated to fake news
words_count <- words_count %>%
  filter(word != c('fake','news'))

head(words_count)
tail(words_count)
```

Visual of top words
```{r}
words_count2 <- subset(words_count, word !="fake")
words_count2 <- subset(words_count2, word !="news")
head(words_count2)

words_count %>%
  filter(word != c("fake", "news"))

p3 <- head(words_count2) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "New York Post Top Words",
         y = "count") +
  theme(legend.position = "none")
```


```{r fig.height=8, fig.width=8}
grid.arrange(p1, p2, p3, ncol = 2)
```



Prep data for sentiment analysis
```{r}
# load data as a corpus
textDoc <- Corpus(VectorSource(master_list_text))

# replace "/", "@", and "|" with space
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
textDoc <- tm_map(textDoc, toSpace, "/")
textDoc <- tm_map(textDoc, toSpace, "@")
textDoc <- tm_map(textDoc, toSpace, "\\|")

# convert the text to lower case
textDoc <- tm_map(textDoc, content_transformer(tolower))
# remove numbers
textDoc <- tm_map(textDoc, removeNumbers)
# remove english common stopwords
textDoc <- tm_map(textDoc, removeWords, stopwords("english"))
# remove punctuation
textDoc <- tm_map(textDoc, removePunctuation)
# eliminate extra white spaces
textDoc <- tm_map(textDoc, stripWhitespace)
# text stemming - reduce words to their root form
textDoc <- tm_map(textDoc, stemDocument)
# remove custom words after seeing the frequency
removeSpecialChars <- function(x) gsub("-","",x)
textDoc <- tm_map(textDoc, removeSpecialChars)
# remove custom words 
textDoc <- tm_map(textDoc, removeWords, c("said","news","fake","post","''"))
```

Build Document Term Matrix (DTM)
```{r}
textDoc_dtm <- TermDocumentMatrix(textDoc)
dtm_mp <- as.matrix(textDoc_dtm)
# sort by decreasing value of frequency
dtm_vp <- sort(rowSums(dtm_mp), decreasing = TRUE)
dtm_dp <- data.frame(word = names(dtm_vp), freq=dtm_vp)
# remove first two rows
dtm_dp <- dtm_dp[-c(1:2),]
# remove last 5 rows
dtm_dp <- slice(dtm_dp, 1:(n() - 6))
# display the top 5 most frequent works
head(dtm_dp)
tail(dtm_dp)
```

plot most frequent words
```{r}
barplot(dtm_dp[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col = "lightgreen", main = "Top 5 most frequent words", 
        ylab = "Word Frequencies")
```

Generate word cloud
```{r}
set.seed(1234)
wordcloud(words = dtm_df$word, freq = dtm_df$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
title("Fake News Dataset")

wordcloud(words = dtm_dr$word, freq = dtm_dr$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
title("Real News Dataset")

wordcloud(words = dtm_dp$word, freq = dtm_dp$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
title("New York Post Dataset")
```

Find word association
```{r eval=FALSE, include=FALSE}
findAssocs(textDoc_dtm, terms = c("report","say","trump"), corlimit = 0.25)
```

Find associations for words that occur at least 50 times
```{r eval=FALSE, include=FALSE}
findAssocs(textDoc_dtm, terms = findFreqTerms(textDoc_dtm, lowfreq = 50), corlimit = 0.25)
```

## Sentiment Analysis 

Sentiment Scores NYP
```{r}
# clean raw file before getting sentiment scores
clean_master_list <- master_list_text
clean_master_list$text <- gsub("\\n"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("\\t"," ",as.character(clean_master_list$text))
clean_master_list$text <- gsub("More On:"," ",as.character(clean_master_list$text))
head(clean_master_list)
```

The different sentiment score methods
Syuzhet (anger, disgust, fear, joy, sadness, surprise)
Bing (positive, negative)
Afinn (positive, negative)
Sentiment scores for NY Post data

```{r}
syuzhet_vector <- get_sentiment(clean_master_list$text, method = "syuzhet")
bing_vector <- get_sentiment(clean_master_list$text, method = "bing")
afinn_vector <- get_sentiment(clean_master_list$text, method = "afinn")
summary(syuzhet_vector)
summary(bing_vector)
summary(afinn_vector)
```

Emotional classification
```{r}
d <- get_nrc_sentiment(clean_master_list$text)
head(d)
```

Graph the emotional classification results
```{r}
#transpose
td <- data.frame(t(d))
# compute sums of each column
td_new <- data.frame(rowSums(td[2:200]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2 <- td_new[1:10,]
quickplot(sentiment, data = td_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("NYP Sentiments")
q3 <- quickplot(sentiment, data = td_new2, weight=count, geom = "bar", fill=sentiment, ylab = "count") + ggtitle("NYP Sentiments")
```

% of emotions
```{r}
barplot(
  sort(colSums(prop.table(d[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in New York Post", xlab = "Percentage",
)

```

N-gram analysis
```{r}
?bigram
```

## Topic Modeling NYP

```{r}
head(master_list_text)

tokens <- master_list_text$text %>%
  tokens(what = "word",
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english"))

dfm <- dfm_trim(dfm(tokens), min_termfreq = 0.005, max_docfreq = 0.99,
                docfreq_type = "prop", verbose = TRUE)

dfm
```

Check the top 20 features
```{r}
topfeatures(dfm, n=20, scheme = "docfreq")
```

Remove additional words and check top features again. 
```{r}
dfm <- dfm_remove(dfm, c("news","fake","$","ii"))
topfeatures(dfm, n=20, scheme = "docfreq")
```


# Statistical Methods NYP

For this topic modeling, we will use and Latent Dirichlet Allocation (LDA) model.

Convert the DTM into a topicmodel so we can run the LDA model. 
```{r}
dtm <- convert(dfm, to = "topicmodels")
set.seed(1)
m <- LDA(dtm, method = "Gibbs", k = 15, control = list(alpha = 0.1))
#n <- LDA(dtm, method = "Gibbs", k = 15, control = list(seed=1234))
m
#n
```

Inspecting the LDA results by looking at the top 5 terms for 10 Topics
```{r}
terms(m, 5)
#terms(n,5)
```

Visualization of the top topics
```{r}
#tidy_lda <- as.data.frame(terms(m, dim(m)[1]))
tidy_lda <- tidy(m)
tidy_lda
```

```{r}
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```


```{r fig.height=15, fig.width=10}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - New York Post", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```


# Train/Test Data

## NYP data

```{r}
# load data as a corpus
textDoc_p <- Corpus(VectorSource(master_list_text))

# replace "/", "@", and "|" with space
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
textDoc <- tm_map(textDoc_p, toSpace, "/")
textDoc <- tm_map(textDoc_p, toSpace, "@")
textDoc <- tm_map(textDoc_p, toSpace, "\\|")

# convert the text to lower case
textDoc_p <- tm_map(textDoc_p, content_transformer(tolower))
# remove numbers
textDoc_p <- tm_map(textDoc_p, removeNumbers)
# remove english common stopwords
textDoc_p <- tm_map(textDoc_p, removeWords, stopwords("english"))
# remove punctuation
textDoc_p <- tm_map(textDoc_p, removePunctuation)
# eliminate extra white spaces
textDoc_p <- tm_map(textDoc_p, stripWhitespace)
# text stemming - reduce words to their root form
textDoc_p <- tm_map(textDoc_p, stemDocument)
# remove custom words after seeing the frequency
removeSpecialChars <- function(x) gsub("-","",x)
textDoc_p <- tm_map(textDoc_p, removeSpecialChars)
# remove custom words 
textDoc_p <- tm_map(textDoc_p, removeWords, c("said","news","fake","post","''"))
```


```{r}
#create sample DTM for NYP
p_dtm <- DocumentTermMatrix(textDoc_p)

#set number of topics
#p_topics <- 5

#split data into train and test sets 70/30
set.seed(123)
train_index_p <- sample(1:nrow(p_dtm), size = 0.7 * nrow(p_dtm))
train_dtm_p <- p_dtm[train_index_p,]
test_dtm_p <- p_dtm[-train_index_p,]

#train LDA model on training set
lda_model_p <- LDA(train_dtm_p, method = "Gibbs", k = 5, control = list(alpha = 0.01))

#calculate perplexity for test
perplexity(lda_model_p, test_dtm_p)

# extract the topics for the test sest
#test_topics_p <- posterior(lda_model_p, newdata = test_dtm_p)$topics

#print(test_topics_p)

print(p_dtm)
```


# Results

We can evaluate the results of the LDA models using human judgement, quantitative metrics, or a combination of the two. The top words in each dataset were evaluated for Word Intrusion, a word within the group that did not belong to the topic. For example, the words Fake, News, and Post were removed the from the New York Post dataset after looking at the top topics. The articles were scraped from a section of the New York Post website with the takes Fake News so seeing these words as top terms does not help us in our analysis. By removing these words we can see the actual topics that are important in this dataset. Human judgement was used identify terms that should be removed when evaluating the top 10 terms from a sample of 15 topics.

The quantitative metric for evaluating an LDA model is Perplexity, which calculates the 'held our likelihood'. We will look at the perplexity of each model. 

https://highdemandskills.com/topic-model-evaluation/ 

```{r}

```




# Conclusion

We cannot predict if an article is Fake but this method allows us to have a better understanding of properties and topics in a fake article. With the sentiment analysis we can see the "mood" of these articles. So we take the LDA model results from the known Fake News dataset and compare it to the data scraped from the New York Post. Even though the NYP does not fall under the definition of "Fake" news for the purposes of this project, the articles were written to identify fake news on the web, so the topics in the NYP are still useful in this analysis. 



```{r fig.height=6, fig.width=13}
grid.arrange(p1, p2, p3, ncol = 2)
```




# Abstract

# Reference



# Appendix

Top Words
```{r fig.height=6, fig.width=13}
grid.arrange(p1, p2, p3, ncol = 3)
```

```{r}
head(dtm_df)

# read dataset dtm
head(dtm_dr)
# new york post dtm
head(dtm_dp)
```



Word Clouds
```{r fig.width=10, message=FALSE, warning=FALSE}
set.seed(1234)
wordcloud(words = dtm_df$word, freq = dtm_df$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2")) + title("Fake News Dataset")



wordcloud(words = dtm_dr$word, freq = dtm_dr$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2")) + title("Real News Dataset")



wordcloud(words = dtm_dp$word, freq = dtm_dp$freq, min.freq = 10, max.words = 100, random.order = FALSE,
          rot.per = 0.40, colors = brewer.pal(8, "Dark2")) + title("New York Post Dataset")

```

Sentiment Analysis comparison
```{r fig.width=8, fig.height=8}
grid.arrange(q1, q2, q3, ncol = 1)
```

Sentiment % Comparison
```{r }
barplot(
  sort(colSums(prop.table(f[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in Fake News Dataset", xlab = "Percentage",
)

```

```{r}
barplot(
  sort(colSums(prop.table(r[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in Real News Dataset", xlab = "Percentage",
)
```

```{r}
barplot(
  sort(colSums(prop.table(d[,1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions % in New York Post", xlab = "Percentage",
)
```


LDA Model Comparison

Fake Data
```{r fig.height=15, fig.width=10}
top_terms_f %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - Fake News Dataset", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```


Real Data
```{r fig.height=15, fig.width=10}
top_terms_r %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - Real News Dataset", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```


NYP Data
```{r fig.height=15, fig.width=10}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%
  arrange(desc(beta)) %>%
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic - New York Post", 
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 3, scales = "free")
```




