---
title: "Real vs Fake News"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction


# Literature Review



# Hypothesis or Reasearch Question


# Data and Variables

```{r}
# libraries
library(quanteda)
library(dplyr)
library(tidytext)
library(tm)
library(topicmodels)
library(data.table)
library(ggplot2)
```


```{r}
# import the data
fake_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/Fake.csv?raw=true")
real_df <- read.csv("https://github.com/ltcancel/DATA698/blob/main/Data/True.csv?raw=true")
```

```{r}
# preview raw data
head(fake_df)
head(real_df)
summary(fake_df)
str(real_df)
```

Other summaries
```{r}
fake_df %>%
  count(subject, sort = TRUE)

real_df %>%
  count(subject, sort = TRUE)
```




```{r}
# clean the data  using tidytext
fake_words <- fake_df %>%
  unnest_tokens(word, text)

# remove stop words
fake_words <- fake_words %>% anti_join(stop_words, by = "word")

# count each word
fake_count <- fake_words %>% 
  count(word, sort = TRUE)
head(fake_count)
```

chart of words
```{r}
head(fake_count) %>%
  ggplot(aes (word, n, fill = word)) +
  geom_col(stat = "identity") + 
  coord_flip() + 
  labs(title = "Fake News Top Words",
         y = "count")
```



Normalizing the data by case-folding, stopword removal, stamming, lemmatization, and contraction simplification
```{r}
head(fake_words)
tail(fake_words)
summary(fake_words)
str(fake_words)



# clean the data using Text Mine (TM)
# create a corpus from the fake_df file
fake_df.corpus <- Corpus(VectorSource(fake_df))
#remove stopwords
#fake.corpus <- tm_map(fake_df.corpus, removeWords, stopwords("english"))
# convert to lower
fake.corpus <- tm_map(fake_df.corpus, tolower)
#remove numbers
fake.corpus <- tm_map(fake_df.corpus, removeNumbers)
#remove punctuations
fake.corpus <- tm_map(fake_df.corpus, removePunctuation)
#strip white spaces
fake.corpus <- tm_map(fake_df.corpus, stripWhitespace)
# stemming
#fake.corpus <- tm_map(fake.corpus, stemDocument)

head(fake.corpus)
```

Create Document Term Matrix (DTM)
```{r}
# first DTM
fake.dtm <- DocumentTermMatrix(fake.corpus)
# remove low frequency words (sparse terms)
fake.dtm2 <- removeSparseTerms(fake.dtm, sparse = 0.96)


#convert to dataframe
fake.dtm.df <- as.data.frame(as.matrix(fake.dtm2))
head(fake.dtm.df)
write.csv(fake.dtm.df,"DTM.csv")
```

```{r}
# sort by decreasing order
x <- mapply(sum, fake.dtm.df)
colSums(fake.dtm.df)
fake.dtm.sort <- data.frame(fake.dtm.sort)
fake.dtm.sort <- order(fake.dtm.sort, decreasing = TRUE)

fake.dtm.sort
#transpose df
summary(fake.dtm.sort)
fake.dtm.d <- data.frame(word = names(fake.dtm.sort), freq=fake.dtm.sort)
head(fake.dtm.d)
```


Topic modeling. Using latent Dirichlet allocation (LDA) to learn the essential words in our DTM
```{r}
dtm_s <- sort(rowSums())

# LDA 
fake.lda <- LDA(fake.dtm2, k = 2, control = list(seed = 1234))


?LDA
```


# Statistical Methods



# Reference



# Appendix


# Results


# Conclusion



# Abstract






